<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Chapter 26 Tutorial</title>
  <style>
    body { font-family: Arial, sans-serif; line-height: 1.5; margin: 20px; }
    h2 { margin-top: 30px; }
    h3 { margin-bottom: 5px; }
    ul { margin-top: 0; }
    .formula { font-family: "Courier New", monospace; background: #f5f5f5; padding: 4px 6px; display: inline-block; }
  </style>
</head>
<body>
  <h2>26.1 What is Multiple Regression?</h2>
  <h3>Key ideas</h3>
  <ul>
    <li>Multiple regression allows several predictors to explain a single response by choosing coefficients that minimize the sum of squared residuals.</li>
    <li>Regression output still reports the coefficient, its standard error, a t-statistic, and a P-value, plus global summaries such as R<sup>2</sup>, the residual standard deviation s, and the degrees of freedom.</li>
    <li>Residuals measure the distance from each observation to the fitted surface and remain central for diagnostics.</li>
  </ul>
  <h3>Important formulas &amp; their meaning</h3>
  <ul>
    <li>
      <span class="formula">\hat{y}=b_0+b_1x_1+b_2x_2+\cdots+b_kx_k</span><br />
      This gives the model’s prediction for y (same units as the response); each coefficient shows the average change in y per unit change in the corresponding predictor while holding the others fixed.
    </li>
    <li>
      <span class="formula">Residual=y-\hat{y}</span><br />
      The residual has the same units as y and quantifies how far a case lies from the fitted value after accounting for all predictors.
    </li>
    <li>
      <span class="formula">R^2=1-\dfrac{SS_{\text{Residual}}}{SS_{\text{Total}}}</span><br />
      This dimensionless ratio indicates the proportion of y’s variability explained by the model; it can only stay the same or increase when you add predictors.
    </li>
  </ul>
  <h3>Short example(s)</h3>
  <ul>
    <li>
      The real estate model <em>Price</em> = 308,100 + 135&amp;middot;LivingArea − 43,347&amp;middot;Bedrooms has R<sup>2</sup>=14.6% and s=266,899. Despite the low R<sup>2</sup>, the tiny P-values tell us both slopes are unlikely to be zero, and the model still gives conditional insights.
    </li>
  </ul>

  <h2>26.2 Interpreting Multiple Regression Coefficients</h2>
  <h3>Key ideas</h3>
  <ul>
    <li>Each coefficient now reflects a conditional association: it measures how y changes with a predictor after adjusting for the other x’s.</li>
    <li>Partial regression plots (added-variable plots) display the residuals of y and of the predictor after regressing both on the remaining variables; their slope equals the coefficient of interest.</li>
  </ul>
  <h3>Important formulas &amp; their meaning</h3>
  <ul>
    <li>
      Partial regression slope equals the coefficient:<br />
      When you plot the residuals of y (after removing other predictors) against the residuals of x<sub>j</sub> (after removing the same predictors), the slope equals b<sub>j</sub>. This slope quantifies the conditional change in y per unit of x<sub>j</sub> and carries the units of y per unit of x<sub>j</sub>.
    </li>
  </ul>
  <h3>Short example(s)</h3>
  <ul>
    <li>
      In the body fat model, Height’s coefficient is −0.60. The partial regression plot shows that among men with a fixed waist size, each extra inch of height is associated with a 0.6% lower body fat—an effect that the simple regression could not detect.
    </li>
  </ul>

  <h2>26.3 Model Assumptions and Conditions</h2>
  <h3>Key ideas</h3>
  <ul>
    <li>Check the Straight Enough Condition for each predictor, examine residual plots for independence and equal spread, and verify the Nearly Normal Condition with histograms or probability plots.</li>
    <li>Because multiple regression still supposes a linear relationship, the plots should show no curvy patterns; any thickening or bending signals reconsideration of transforms or variables.</li>
  </ul>
  <h3>Important formulas &amp; their meaning</h3>
  <ul>
    <li>
      <span class="formula">y = b_0 + b_1x_1 + \cdots + b_kx_k + e</span><br />
      This writes the full model including the error term e, which should be independent, have constant spread, and (for inference) be nearly Normal. The fitted value excludes e.
    </li>
    <li>
      <span class="formula">s = \sqrt{\dfrac{SS_{\text{Residual}}}{n - (k + 1)}}</span><br />
      The residual standard deviation s estimates the typical forecast error in the units of y and depends on the total number of coefficients (including the intercept).
    </li>
  </ul>
  <h3>Short example(s)</h3>
  <ul>
    <li>
      The step-by-step body fat analysis checked scatterplots for waist and height, confirmed no pattern/intensity change in the residual vs. fitted plot, and displayed a nearly Normal histogram and probability plot—so all four conditions were satisfied before inference.
    </li>
  </ul>

  <h2>26.4 Multiple Regression Inference</h2>
  <h3>Key ideas</h3>
  <ul>
    <li>First test whether the model as a whole improves on the mean alone with the global F-test; proceed to examine individual t-tests only if you reject the null.</li>
    <li>Individual coefficients are tested with Student’s t-distribution using n − k − 1 degrees of freedom, and each confidence interval follows the usual estimate ± margin of error structure.</li>
  </ul>
  <h3>Important formulas &amp; their meaning</h3>
  <ul>
    <li>
      <span class="formula">F = \dfrac{MS_{\text{Regression}}}{MS_{\text{Residual}}}</span><br />
      The F-statistic compares the explained variance per predictor to the unexplained variance per error degree of freedom; a large value (and small P-value) means the full model beats just using the mean.
    </li>
    <li>
      <span class="formula">t = \dfrac{b_j - 0}{\text{SE}(b_j)}</span><br />
      Each coefficient’s t-statistic measures how many standard errors the estimate is from zero; a large magnitude signals that x<sub>j</sub> contributes after the others are in the model.
    </li>
    <li>
      <span class="formula">b_j \pm t^*_{\alpha/2,\,n-k-1} \cdot \text{SE}(b_j)</span><br />
      A confidence interval for b<sub>j</sub> quantifies the range of plausible true values, acknowledging sampling variation. The units match those of the slope b<sub>j</sub>.
    </li>
  </ul>
  <h3>Short example(s)</h3>
  <ul>
    <li>
      The infant mortality regression gave F=21.8 (p&lt;0.0001), so the model is useful overall. Among the coefficients, Child Deaths (t=2.25, p=0.029) and Low Birth Weight (t=5.56, p&lt;0.0001) were significantly different from zero, while the other predictors were not.
    </li>
  </ul>

  <h2>26.5 Comparing Multiple Regression Models</h2>
  <h3>Key ideas</h3>
  <ul>
    <li>Because R<sup>2</sup> cannot decrease, it’s risky to pick models based solely on the highest value—extra variables can inflate R<sup>2</sup> without meaningful contribution.</li>
    <li>Use adjusted R<sup>2</sup> or drop-in-predictor F-tests when comparing models with different numbers of predictors, and always weigh the substantive meaning of variables.</li>
  </ul>
  <h3>Important formulas &amp; their meaning</h3>
  <ul>
    <li>
      <span class="formula">R^2=1-\dfrac{SS_{\text{Residual}}}{SS_{\text{Total}}}</span><br />
      Keeps describing how much of the total variation the model explains, but it never penalizes adding more predictors.
    </li>
    <li>
      <span class="formula">R^2_{\text{adj}}=1-\dfrac{MS_{\text{Residual}}}{MS_{\text{Total}}}</span><br />
      Because mean squares divide sums of squares by degrees of freedom, this adjusted statistic can fall when you add useless predictors, making it better suited for model comparison.
    </li>
    <li>
      <span class="formula">F_{\text{drop}}=\dfrac{SS_{\text{Drop}}/(k-g)}{MS_{\text{Residual, Complete}}}</span><br />
      To test whether a set of k−g predictors adds useful information beyond a simpler model, compare the extra explained variation to the residual variation from the full model.
    </li>
  </ul>
  <h3>Short example(s)</h3>
  <ul>
    <li>
      Dropping HS Drop%, Teen Births, and Teen Deaths from the infant mortality model increased the residual sum of squares by 2.054, giving F_{\text{drop}}=1.816 (p≈0.17); the large P-value and unchanged adjusted R<sup>2</sup> imply those three variables do not improve the model beyond Low Birth Weight and Child Deaths.
    </li>
  </ul>
</body>
</html>
