<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>Chapter 11: From Randomness to Probability</title>
  <style>
    body { font-family: Georgia, 'Times New Roman', serif; line-height: 1.6; padding: 2rem; max-width: 900px; margin: auto; }
    h2 { border-bottom: 2px solid #333; padding-bottom: 0.25rem; }
    h3 { margin-top: 1.5rem; }
    ul { margin-top: 0.5rem; }
    li + li { margin-top: 0.35rem; }
    pre { background-color: #f4f4f4; padding: 0.75rem; border-radius: 6px; overflow-x: auto; }
    strong { color: #333; }
  </style>
</head>
<body>
  <h2>11.1 Random Phenomena</h2>
  <h3>Key ideas</h3>
  <ul>
    <li>Random phenomena occur when we know the possible outcomes (the sample space) but cannot predict which one will happen on any trial.</li>
    <li>Trials produce outcomes that combine into events, and the sample space <em>S</em> lists every outcome.</li>
    <li>The Law of Large Numbers guarantees that repeated independent, identical trials make an event’s relative frequency settle near a fixed probability.</li>
    <li>Short-run swings do not influence future trials—the “Law of Averages” is a misconception.</li>
    <li>Independence means no trial affects another, a requirement for applying LLN reliably.</li>
  </ul>

  <h3>Important formulas &amp; their meaning</h3>
  <ul>
    <li><strong>Random phenomenon</strong><br />
      A random phenomenon is a situation where we can list all possible outcomes but cannot predict which one will occur on a particular trial. Think of driving to a light that could show red, yellow, or green: the set of outcomes is clear, but the next colour is not. Each trial is one arrival, and its outcome is the colour.</li>

    <li><strong>Law of Large Numbers (LLN)</strong><br />
      As the number of independent, identical trials grows, the relative frequency of an event converges to the event’s probability. This explains why long-run averages settle near a constant—after many recordings at Portage and Main the percent green light stabilizes near 35%.</li>

    <li><strong>Empirical probability</strong><br />
      \(P(A) = \dfrac{\text{# times event }A \text{ occurs}}{\text{total # of trials}}\). The numerator counts how often \(A\) happens, the denominator counts every trial, and the ratio gives a unitless probability between 0 and 1 based on observation. Example: 3 green lights in 6 trials gives \(P(\text{green}) = 0.5\).</li>

    <li><strong>Independence (informal)</strong><br />
      Trials are independent if one trial’s outcome has no influence on another’s outcome. A coin flip after six heads still has \(P(\text{heads}) = 0.5\); the coin has no memory.</li>
  </ul>

  <h3>Short example(s)</h3>
  <ul>
    <li>Recording the traffic light over several days shows the percent green starting at 100%, dipping after red lights, and eventually settling near 35%, illustrating LLN and empirical probability.</li>
    <li>Simulating 100,000 coin flips still produces stochastic streaks (e.g., 5 heads in a row), demonstrating that short-term patterns offer no leverage over future outcomes.</li>
  </ul>

  <h2>11.2 Modelling Probability</h2>
  <h3>Key ideas</h3>
  <ul>
    <li>Theoretical probability models assume equally likely outcomes, so probabilities come from counting favourable outcomes within the sample space.</li>
    <li>Every probability must lie between 0 and 1, and the probabilities of all outcomes in <em>S</em> must sum to 1 for the assignment to be legitimate.</li>
    <li>Rules such as the Complement Rule and Addition Rules help combine events even when they overlap.</li>
  </ul>

  <h3>Important formulas &amp; their meaning</h3>
  <ul>
    <li><strong>Theoretical probability</strong><br />
      \(P(A) = \dfrac{\text{\# outcomes in }A}{\text{\# outcomes in }S}\) when outcomes are equally likely. Counting ratios replaces experimentation in such models. For drawing a face card: \(12/52 = 3/13 \approx 0.23\).</li>

    <li><strong>Probability bounds &amp; assignment</strong><br />
      All probabilities satisfy \(0 \le P(A) \le 1\) because they represent proportions, and \(P(S) = 1\) because something must happen. A spinner’s probabilities must add to 1; if three colours sum to 0.70, the fourth must cover 0.30.</li>

    <li><strong>Complement Rule</strong><br />
      \(P(A^c) = 1 - P(A)\). The event “not \(A\)” fills in the rest of the probability space, so probabilities sum to 1. Example: if \(P(\text{rain}) = 0.25\), then \(P(\text{no rain}) = 0.75\).</li>

    <li><strong>Addition Rule for disjoint events</strong><br />
      For disjoint \(A\) and \(B\), \(P(A \text{ or } B) = P(A) + P(B)\). Since they cannot happen together, their probabilities just add. Example: drawing a heart or a club gives \(1/4 + 1/4 = 1/2\).</li>

    <li><strong>General Addition Rule</strong><br />
      \(P(A \text{ or } B) = P(A) + P(B) - P(A \text{ and } B)\). Overlapping events require subtracting the intersection to avoid double counting. Example: homes with a garage (64%) or a pool (21%) but both (17%) gives \(0.64 + 0.21 - 0.17 = 0.68\).</li>
  </ul>

  <h3>Short example(s)</h3>
  <ul>
    <li>Drawing cards from a standard deck demonstrates theoretical probability: with 52 equally likely cards, the chance of an ace is \(4/52 = 1/13\).</li>
    <li>Census percentages for filing methods show how to split probability mass across overlapping categories while applying complement and addition rules.</li>
    <li>Probabilities for spinner colours or discount cards must stay within [0,1] and sum to 1; values like 1.20 or negative numbers immediately violate legitimacy.</li>
  </ul>
</body>
</html>
